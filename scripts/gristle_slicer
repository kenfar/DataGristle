#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is ':' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is ':' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.

    --max-mem-recs RECS The total number of recs to keep in memory.  If the number of
                        records in the file is greater than this it will use a slower
                        process.

Notes:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset'
    This specification is either a comma-delimited list of individual offsets or ranges.
    Offsets are based on zero, and if negative are measured from the end of the record
    or file with -1 being the final item.  There can be N number of individual offsets.
    Ranges are a pair of offsets separated by a colon.  The first number indicates the
    starting offset, and the second number indicates the stop offset +1.


{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2021 Ken Farmer
"""
import csv
import errno
import fileinput
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
from typing import List, Tuple, Dict, Any, Optional, IO

import datagristle.common as comm
import datagristle.configulator as conf
import datagristle.csvhelper as csvhelper
import datagristle.file_io as file_io
import datagristle.helpdoc as helpdoc
import datagristle.location_slicer as slicer

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)
comm.validate_python_version()



def main() -> int:

    try:
        config_manager = ConfigManager(NAME, SHORT_HELP, LONG_HELP)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA

    input_handler = file_io.InputHandler(nconfig.infiles,
                                         nconfig.dialect,
                                         return_header=True)

    # Set up the merged specs (resolves inclusion & exclusion specs), as
    # well as the record slicers.  We need these record slicers because
    # there may be too many recs to keep the merged_rec_spec in memory.

    (incl_rec_slicer,
     excl_rec_slicer,
     merged_rec_spec,
     valid_rec_spec) = setup_rec_slicers(nconfig.infiles,
                                          input_handler.dialect,
                                          nconfig.records,
                                          nconfig.exrecords,
                                          nconfig.max_mem_recs)

    (incl_col_slicer,
     excl_col_slicer,
     merged_col_spec,
     valid_col_spec) = setup_col_slicers(nconfig.infiles,
                                          nconfig.columns,
                                          nconfig.excolumns,
                                          nconfig.header)


    output_handler = file_io.OutputHandler(nconfig.outfile, input_handler.dialect)

    # If the number of recs is small, then process in memory.  Otherwise, use
    # the rec slicers and read one rec at a time and process.
    if (nconfig.infiles[0] == '-'
            or valid_rec_spec is False
            or valid_col_spec is False):
        process_recs_from_file(input_handler,
                               output_handler,
                               incl_rec_slicer,
                               excl_rec_slicer,
                               incl_col_slicer,
                               excl_col_slicer)
    else:
        process_recs_in_memory(input_handler,
                               output_handler,
                               merged_rec_spec,
                               merged_col_spec)
    input_handler.close()
    output_handler.close()

    return 0




def setup_rec_slicers(infiles: List[str],
                      dialect: csv.Dialect,
                      config_records: str,
                      config_exrecords: str,
                      max_mem_recs: int) -> Tuple[slicer.SpecProcessor,
                                                  slicer.SpecProcessor,
                                                  List[int],
                                                  bool]:
    """  Sets up the slicer objects (inclusion & exclusion) for
         recs as well as the merged_recs_specs for columns.

         Then creates a merged_rec_spec - which is the exact list of recs
         to slice if possible.  If there's insufficient memory then this
         list will be None.
    """
    # first parse the config items;
    records   = config_records.split(',')
    exrecords = config_exrecords.split(',') if config_exrecords else []

    # Set up row slicing:
    # Note: we don't have to worry that we're going to count records for stdin
    #       since we've already validated this.
    # Note: Consider not counting this now, but instead let the SpecProcessor
    #       return no expanded_specs if unbounded
    rec_cnt = get_rec_count(infiles, dialect)

    incl_rec_slicer = slicer.SpecProcessor(records,
                                           header=None,
                                           infile_item_count=rec_cnt,
                                           max_mem_recs=max_mem_recs)
    excl_rec_slicer = slicer.SpecProcessor(exrecords,
                                           header=None,
                                           infile_item_count=rec_cnt,
                                           max_mem_recs=max_mem_recs)

    (valid_merged_spec, merged_rec_spec) = merge_expanded_specs(
                                           incl_rec_slicer.expanded_specs,
                                           incl_rec_slicer.expanded_specs_valid,
                                           excl_rec_slicer.expanded_specs,
                                           incl_rec_slicer.expanded_specs_valid)
    return (incl_rec_slicer,
            excl_rec_slicer,
            merged_rec_spec,
            valid_merged_spec)



def setup_col_slicers(infiles: List[str],
                      config_columns: str,
                      config_excolumns: str,
                      config_header: csvhelper.Header) -> Tuple[slicer.SpecProcessor,
                                                                slicer.SpecProcessor,
                                                                List[int],
                                                                bool]:

    """  Sets up the 2 slicer objects: inclusion & exclusion for
         recs and the merged_col_spec for columns.

         Then creates and returns a merged column spec that contains
         an exact list of columns to slice.
    """
    # first parse the config items;
    columns   = config_columns.split(',')
    excolumns = config_excolumns.split(',') if config_excolumns else []

    # set up the slicers:
    col_count = len(config_header.field_names)-1
    incl_col_slicer = slicer.SpecProcessor(columns,
                                           config_header,
                                           infile_item_count=col_count)
    excl_col_slicer = slicer.SpecProcessor(excolumns,
                                           config_header,
                                           infile_item_count=col_count)

    (valid_merged_spec, merged_col_spec) = merge_expanded_specs(
                                           incl_col_slicer.expanded_specs,
                                           incl_col_slicer.expanded_specs_valid,
                                           excl_col_slicer.expanded_specs,
                                           excl_col_slicer.expanded_specs_valid)
    assert merged_col_spec is not None
    return (incl_col_slicer,
            excl_col_slicer,
            merged_col_spec,
            valid_merged_spec)



def merge_expanded_specs(incl_expanded_specs: List[int],
                         incl_expanded_specs_valid: bool,
                         excl_expanded_specs: List[int],
                         excl_expanded_specs_valid: bool) -> Tuple[bool, List[int]]:
    """ Merge the inclusion & exclusion expansion specs

    These specs are just lists of positions - merge these so that there's a 
    single list to perform lookups against.  Note that the expanded specs will
    be empty if they are too large for memory.
    """
    valid_spec = True
    invalid_spec = False

    if not incl_expanded_specs_valid  or not excl_expanded_specs_valid:
        return (invalid_spec, [])

    merged_specs = []
    for spec_item in incl_expanded_specs:
        if spec_item in excl_expanded_specs:
            continue
        else:
            merged_specs.append(spec_item)
    return (valid_spec, merged_specs)



def get_rec_count(files: List[str],
                  dialect: csv.Dialect) -> int:
    """ Get record counts for input files.
        - Counts have an offset of 0
    """
    rec_cnt = -1

    if files[0] == '-':
        return rec_cnt

    for _ in csv.reader(fileinput.input(files), dialect):
        rec_cnt += 1
    fileinput.close()
    return rec_cnt



def process_recs_from_file(input_handler,
                           output_handler,
                           incl_rec_slicer: slicer.SpecProcessor,
                           excl_rec_slicer: slicer.SpecProcessor,
                           incl_col_slicer: slicer.SpecProcessor,
                           excl_col_slicer: slicer.SpecProcessor) -> None:
                           #merged_col_spec) -> None:
    """ Reads the file one record at a time, compares against the
        specification, and then writes out qualifying records and
        columns.
        Args:
            - input_handler
            - output_handler
            - incl_rec_spec
            - excl_rec_spec
            - merged_col_spec: simple list of which columns to slice
    """

    for rec in input_handler:
        rec_number = input_handler.rec_cnt - 1

        if not incl_rec_slicer.specs_evaluator(rec_number):
            return None
        if excl_rec_slicer.specs_evaluator(rec_number):
            return None

        output_rec = []
        for col_number in range(0, len(rec)):
            if not incl_col_slicer.specs_evaluator(col_number):
                continue
            if excl_col_slicer.specs_evaluator(col_number):
                continue
            output_rec.append(rec[col_number])

        if output_rec:
            output_handler.write_rec(output_rec)



def process_recs_in_memory(input_handler,
                           output_handler,
                           merged_rec_spec,
                           merged_col_spec) -> None:

    """ Reads the entire file into memory, then processes one record
        at a time, compares against the specification, and then writes
        out qualifying records and columns.
        Args:
            - input_handler
            - output_handler
            - merged_rec_spec: a list of all columns from the record
            - merged_col_spec: simple list of which columns to slice
    """
    all_rows = [x for x in input_handler]
    for rec_num in merged_rec_spec:
        output_rec = []
        for col_number in merged_col_spec:
            try:
                output_rec.append(all_rows[rec_num][col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols

        if output_rec:
            output_handler.write_rec(output_rec)




class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_recs',
                                 default=10000,
                                 type=int)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self) -> None:

        self.generate_csv_dialect_config()
        self.generate_csv_header_config()

    def validate_custom_config(self, config) -> None:

        if config['infiles'] == '-':
            if ('-' in config['columns']
                or '-' in config['excolumns']
                or '-' in config['records']
                or '-' in config['exrecords']):
                comm.abort('Error: negative values not supported with stdin')




if __name__ == '__main__':
    sys.exit(main())
