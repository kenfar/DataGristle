#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is ':' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is ':' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.

    --max-mem-recs RECS The total number of recs to keep in memory.  If the number of
                        records in the file is greater than this it will use a slower
                        process.

Notes:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset'
    This specification is either a comma-delimited list of individual offsets or ranges.
    Offsets are based on zero, and if negative are measured from the end of the record
    or file with -1 being the final item.  There can be N number of individual offsets.
    Ranges are a pair of offsets separated by a colon.  The first number indicates the
    starting offset, and the second number indicates the stop offset +1.

Python's indexing rules for a single offset and how gristle_slicer differs:
    1. The offset is based on 0
    2. Negative offsets are based on the end of the list
    3. A positive offset greater than the length of the list results in IndexError
    4. A negative offset that references beyond the front of the list - IndexError

Python's slicing rules for a range and how gristle_slicer differs:
    1. The first number is the inclusive start, the second the exclusive end
    2. The ending offset defaults to length of the list
    3. The starting offset defaults to 0
    4. The ending offset can be longer than the list length - but nothing happens
    5. An impossible range produces an empty string, ex: s[10:2] == ''
    6. the step cannot be 0
    7. combining negs & poss can work: s[-3:9] or s[1:-1]is fine
    8. the starting offset can wrap *once*, ex: s[-4:4] == s[-99999:4]

Python's extended slicing rules and how gristle_slicer differs:
    1. The third item (ex: '2:4:1') is the step, and it describes the increment between
       the start & stop.
    2. The step defaults to 1 - which indicates every record.
    3. Other numbers cause it to step skip records.  Ex: 2:4:2 extracts every 2nd item.
    4. Negative steps cause it to go in reverse.  When using negative steps, the start
       should be the larger number than the stop.  Ex:  4:2:-1

s = [a,b,c,d]
pp(s[::]   == 'dcba'

s = [a,b,c,d]
pp(s[5::-1]   == 'dcba'
pp(s[5:0:-1]  == 'cba'
pp(s[5:-1:-1] == ''         # beyond the right border
pp(s[0:-3:-1] == ''         # wrong order
pp(s[0:-4:-1] == ''         # wrong order
pp(s[0:-5:-1] == 'a'
pp(s[0:-6:-1] == 'a'




{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2022 Ken Farmer
"""
import csv
import errno
import functools
import os
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
import tempfile
import time
from typing import List, Tuple, Dict, Any, Optional, IO, Hashable

import datagristle.common as comm
import datagristle.configulator as conf
from datagristle import file_io
from datagristle import helpdoc
import datagristle.location_slicer as slicer

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)

MAX_MEM_INDEX_CNT = 20_000_000

comm.validate_python_version()


def main() -> int:

    try:
        config_manager = ConfigManager(NAME,
                                       SHORT_HELP,
                                       LONG_HELP,
                                       validate_dialect=False)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA

    slice_runner = SliceRunner(nconfig)
    slice_runner.config_manager = config_manager
    slice_runner.setup_stage1()
    slice_runner.setup_stage2()
    slice_runner.process_data()
    slice_runner.shutdown()

    return 0



class SliceRunner:

    def __init__(self,
                 nconfig) -> None:

        self.nconfig = nconfig
        self.anyorder = False
        self.input_handler: file_io.InputHandler
        self.output_handler: file_io.OutputHandler
        self.temp_fn = None

        self.rec_cnt = None
        self.col_cnt = None
        self.rec_specs: slicer.Specifications
        self.exrec_specs: slicer.Specifications
        self.col_specs: slicer.Specifications
        self.excol_specs: slicer.Specifications
        self.col_default_range = False

        self.incl_rec_slicer: slicer.SpecProcessor
        self.excl_rec_slicer: slicer.SpecProcessor
        self.incl_col_slicer: slicer.SpecProcessor
        self.excl_col_slicer: slicer.SpecProcessor
        self.valid_rec_spec = None
        self.valid_col_spec = None

        self.is_optimized_with_rec_index = False
        self.is_optimized_with_col_index = False
        self.rec_index_optimization_stop_rec = 0

        self.verify_data_recs = []
        self.mem_limiter = comm.MemoryLimiter(max_mem_gbytes=nconfig.max_mem_gbytes)


    def setup_stage1(self) -> None:
        self._setup_files()


    def setup_stage2(self) -> None:
        try:
            self._setup_specs()
        except (slicer.NegativeOffsetWithoutItemCountError,
                slicer.NegativeStepWithoutItemCountError,
                slicer.UnboundedStopWithoutItemCountError) as err:
            if self.is_infiles_from_stdin():
                self._write_stdin_to_file()
                self.nconfig, _ = self.config_manager.get_config(self.temp_fn)
            self._setup_counts()
            try:
                self._setup_specs()
            except (slicer.NegativeOffsetWithoutItemCountError,
                    slicer.NegativeStepWithoutItemCountError,
                    slicer.UnboundedStopWithoutItemCountError):
                comm.abort('Error: unable to count rows in file to resolve config references!',
                        f'Record count: {self.rec_cnt}, Column count: {self.col_cnt}',
                        verbosity='debug')

        self._setup_slicers()
        self._setup_index_optimization()

        self._pp(' ')
        self._pp('Stage2 Optimizations: ')
        self._pp(f'    is_optimized_for_all_recs: {self.is_optimized_for_all_recs()}')
        self._pp(f'    is_optimized_with_rec_index: {self.is_optimized_with_rec_index}')
        self._pp(f'    is_optimized_for_all_cols: {self.is_optimized_for_all_cols()}')
        self._pp(f'    is_optimized_with_col_index: {self.is_optimized_with_col_index}')
        self._pp(f'    rec_index_optimization_stop_rec: {self.rec_index_optimization_stop_rec}')


    def _write_stdin_to_file(self):
        start_time = time.time()

        assert self.nconfig.infiles[0] == '-'
        _, self.temp_fn = tempfile.mkstemp(prefix='gristle_slicer_stdin_temp_')
        with open(self.temp_fn, 'w', newline='', encoding='utf-8') as outbuf:
            writer = csv.writer(outbuf)
            writer.writerows(self.input_handler)
        self.input_handler = file_io.InputHandler([self.temp_fn],
                                                  self.nconfig.dialect,
                                                  return_header=True)

        self._pp(f'--------> write_stdin_to_file duration: {time.time() - start_time:.2f}')


    def _pp(self,
            val: Any) -> None:
        if self.nconfig.verbosity == 'debug':
            print(val)


    def shutdown(self) -> None:
        self.input_handler.close()
        self.output_handler.close()

        if self.temp_fn:
            os.remove(self.temp_fn)


    def _setup_files(self) -> None:
        self.input_handler = file_io.InputHandler(self.nconfig.infiles,
                                                  self.nconfig.dialect,
                                                  return_header=True)
        self.output_handler = file_io.OutputHandler(self.nconfig.outfile,
                                                    self.input_handler.dialect)


    def _setup_counts(self) -> None:
        start_time = time.time()
        #self.estimated_rec_cnt = file_io.get_approx_rec_count(self.nconfig.infiles)
        if self.temp_fn:
            self.rec_cnt = file_io.get_rec_count([self.temp_fn], self.input_handler.dialect)
        else:
            self.rec_cnt = file_io.get_rec_count(self.nconfig.infiles, self.input_handler.dialect)

        self.col_cnt = len(self.nconfig.header.field_names)-1
        if self.col_cnt < 0:
            self.col_cnt = None
        if self.nconfig.verbosity == 'debug':
            print(f'--------> setup_counts  duration: {time.time() - start_time:.2f}')



    def _setup_specs(self) -> None:
        """ Will get run multiple times - as more info trickles in!
        """

        # record specs:
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []
        self.rec_specs = slicer.Specifications('incl_rec',
                                                records,
                                                infile_item_count=self.rec_cnt)
        self.exrec_specs = slicer.Specifications('excl_rec',
                                                 exrecords,
                                                 infile_item_count=self.rec_cnt)

        # col specs:
        records = self.nconfig.columns.split(',')
        exrecords = self.nconfig.excolumns.split(',') if self.nconfig.excolumns else []
        self.col_specs = slicer.Specifications('incl_col',
                                                records,
                                                infile_item_count=self.col_cnt,
                                                header=self.nconfig.header)
        self.excol_specs = slicer.Specifications('excl_col',
                                                 exrecords,
                                                 infile_item_count=self.col_cnt,
                                                 header=self.nconfig.header)


    def _setup_slicers(self) -> None:
        self.incl_rec_slicer = slicer.SpecProcessor(self.rec_specs)
        self.excl_rec_slicer = slicer.SpecProcessor(self.exrec_specs)
        self.incl_col_slicer = slicer.SpecProcessor(self.col_specs)
        self.excl_col_slicer = slicer.SpecProcessor(self.excol_specs)


    def is_optimized_for_all_recs(self) -> bool:
        return bool(self.incl_rec_slicer.has_all_inclusions is True
                    and self.excl_rec_slicer.has_exclusions is False)

    def is_optimized_for_all_cols(self) -> bool:
        return bool(self.incl_col_slicer.has_all_inclusions is True
                    and self.excl_col_slicer.has_exclusions is False)


    def _setup_index_optimization(self) -> None:

        start_time = time.time()

        self.rec_index: List[int] = []
        if (self.incl_rec_slicer.indexer.valid
        and self.excl_rec_slicer.indexer.valid
        and len(self.excl_rec_slicer.index) <= 10_000):
        # Pulling this out until we can figure out how to process in mem in reverse order without index
        #and self.is_optimized_for_all_recs() is False
            for spec_item in self.incl_rec_slicer.index:
                if spec_item in self.excl_rec_slicer.index:
                    continue
                else:
                    if len(self.rec_index) > MAX_MEM_INDEX_CNT:
                        self.rec_index = []
                        break
                    self.rec_index.append(spec_item)
            else:
                self.rec_index_optimization_stop_rec = max(self.rec_index, default=0)
                self.is_optimized_with_rec_index = True

        self.col_index: List[int] = []
        if (self.incl_col_slicer.indexer.valid
        and self.excl_col_slicer.indexer.valid
        and self.is_optimized_for_all_cols() is False):
            for spec_item in self.incl_col_slicer.index:
                if spec_item in self.excl_col_slicer.index:
                    continue
                else:
                    if len(self.col_index) > MAX_MEM_INDEX_CNT:
                        self.col_index = []
                        break
                    self.col_index.append(spec_item)
            else:
                self.is_optimized_with_col_index = True
        self.col_default_range = self.incl_col_slicer.indexer.col_default_range


        self._pp(' ')
        self._pp('Index Optimizations: ')
        self._pp(f'    {self.incl_rec_slicer.has_all_inclusions=}')
        self._pp(f'    {self.incl_rec_slicer.indexer.valid=}')
        self._pp(f'    {len(self.incl_rec_slicer.index)=}')
        self._pp(f'    {self.incl_rec_slicer.includes_out_of_order=}')
        self._pp(f'    {self.incl_rec_slicer.includes_repeats=}')
        self._pp(f'    {self.incl_rec_slicer.includes_reverse=}')
        self._pp(' ')
        self._pp(f'    {self.excl_rec_slicer.has_all_inclusions=}')
        self._pp(f'    {self.excl_rec_slicer.indexer.valid=}')
        self._pp(f'    {len(self.excl_rec_slicer.index)=}')
        self._pp(' ')
        self._pp(f'    {self.incl_col_slicer.indexer.valid=}')
        self._pp(f'    {self.excl_col_slicer.indexer.valid=}')
        self._pp(f'--------> setup_index_optimization  duration: {time.time() - start_time:.2f}')


    def prune_col_index(self,
                        actual_col_cnt: int):
        new_index = []
        new_index = [x for x in self.col_index if x <= actual_col_cnt]
        self.col_index = new_index
        self.col_default_range = False


    def process_data(self) -> None:
        start_time = time.time()
        if self.must_process_in_memory():
            self.process_recs_in_memory()
        else:
            self.process_recs_from_file()
        self._pp(f'--------> process_data duration: {time.time() - start_time:.2f}')


    def must_process_in_memory(self) -> bool:

        # Problem: the only way we know if its out of order is with the index.  If the index build fails, we don't know!
        if (self.incl_rec_slicer.includes_out_of_order
                or self.incl_rec_slicer.includes_repeats
                or self.incl_rec_slicer.includes_reverse):

            if self.is_optimized_with_rec_index:
                return True
            elif self.is_optimized_for_all_recs():
                return True
            else:
                comm.abort('Error: There are out of order, reverse, or repeating rec specs but cannot fit into memory!',
                verbosity='debug')

        return False



    def process_recs_from_file(self) -> None:
        """ Reads the file one record at a time, compares against the
            specification, and then writes out qualifying records and
            columns.
            Args:
                - input_handler
                - output_handler
                - incl_rec_spec
                - excl_rec_spec
                - merged_rec_spec: simple list of which recs to slice
                - merged_col_spec: simple list of which columns to slice
        """
        self._pp(f'process: process_recs_from_file')
        next_index_sub = 0

        for rec_number, rec in enumerate(self.input_handler):

            #if rec_number % 1000 == 0:
            #    self._pp(rec_number)
            if self.is_optimized_for_all_recs():
                pass
            elif self.is_optimized_with_rec_index:
                if rec_number > self.rec_index_optimization_stop_rec:
                    if self.is_infiles_from_stdin():
                        # Need to finish reading from file rather than break so that we don't
                        # break pipe for pgm piping data to us.  We'll spin thru the rest of 
                        # the recs as quickly as possible, doing it all right here is about 20%
                        # faster than going thru the main loop:
                        break
                        for _ in self.input_handler:
                            pass
                        break
                    else:
                        break

                # This only works here because:
                #  1. the rec spec must not be out of order - it's a requirement of process_recs_from_file
                #  2. the rec spec must not be reverse order
                #  3. or the anyorder option was provided
                #if rec_number % 10000 == 0:
                #    self._pp(f'index lookup: {rec_number=}, {next_index_sub=}')

                if rec_number == self.rec_index[next_index_sub]:
                    next_index_sub += 1
                else:
                    continue

            else:
                if not self.incl_rec_slicer.specs_evaluator(rec_number):
                    continue
                elif self.excl_rec_slicer.specs_evaluator(rec_number):
                    continue

            output_rec = []

            if self.is_optimized_for_all_cols():
                output_rec = rec
            elif self.is_optimized_with_col_index:
                output_rec = self.get_cols_from_index(rec,
                                                      self.col_index)
                if self.col_default_range:
                    self.prune_col_index(actual_col_cnt=len(rec))
            else:
                output_rec = self.get_cols_from_eval(rec,
                                                     len(rec),
                                                     self.incl_col_slicer,
                                                     self.excl_col_slicer)

            if output_rec:
                self.output_handler.write_rec(record=output_rec)



    def process_recs_in_memory(self) -> None:
        """ Reads the entire file into memory, then processes one record
            at a time, compares against the specification, and then writes
            out qualifying records and columns.
        """
        self._pp(f'process: process_recs_in_memory')

        if self.incl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'incl_rec.indexer.invalid')
        if self.excl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'excl_rec.indexer.invalid')

        all_rows = []

        rec: List[str] = []
        for rec_number, rec in enumerate(self.input_handler):
            if rec_number > self.rec_index_optimization_stop_rec:
                if self.is_infiles_from_stdin():
                    for _ in self.input_handler:
                        pass
                    break
                else:
                    break
            all_rows.append(rec)
            try:
                self.mem_limiter.check_record(rec, rec_number)
            except MemoryError:
                comm.abort('ERROR: too many rows to fit into memory',
                            'Change options to either process_by_file or break the process into multiple steps')

        # if we originally couldn't get col counts because of stdin,
        # update now that we have it all loaded into a list:
        if self.col_cnt in (None, -1):
            self.col_cnt = len(all_rows[0]) - 1
        assert self.col_cnt > -1

        for rec_num in self.rec_index:
            output_rec = []
            try:
                if self.is_optimized_for_all_cols():
                    output_rec = all_rows[rec_num]
                elif self.is_optimized_with_col_index:
                    output_rec = self.get_cols_from_index(all_rows[rec_num],
                                                          self.col_index)
                else:
                    # The slower fallback eval solution:
                    output_rec = self.get_cols_from_eval(all_rows[rec_num],
                                                         self.col_cnt,
                                                         self.incl_col_slicer,
                                                         self.excl_col_slicer)
            except IndexError:
                pass
            if output_rec:
                self.output_handler.write_rec(record=output_rec)



    def get_cols_from_index(self,
                            input_rec: List[str],
                            col_index: List[int]) -> List[str]:
        """ Slightly faster solution (about 20% faster) than evals, but requires
            more memory.
        """
        output_rec = []
        for col_number in col_index:
            try:
                output_rec.append(input_rec[col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols
        return output_rec


    def get_cols_from_eval(self,
                           input_rec: List[str],
                           col_count: int,
                           incl_col_slicer: slicer.SpecProcessor,
                           excl_col_slicer: slicer.SpecProcessor) -> List[str]:
        """ Primarily used for unbounded col ranges with stdin
            About 20% slower than get_cols_from_index()
            WARNING: is this actually safe?  does it process in the right order?
        """
        #fixme: do we need to ensure that this is only used with cols in order?!?!?!?!
        output_rec = []
        for col_number in range(0, col_count):
            if self.cached_col_eval(col_number, incl_col_slicer, excl_col_slicer):
                output_rec.append(input_rec[col_number])
        return output_rec


    @functools.lru_cache
    def cached_col_eval(self,
                        col_number: int,
                        incl_col_slicer,
                        excl_col_slicer) -> bool:
        """ The caching on this eval speeds it up 50-75% on large files
            with many columns
        """
        if incl_col_slicer.specs_evaluator(location=col_number):
            if not excl_col_slicer.specs_evaluator(location=col_number):
                return True
        return False


    def is_infiles_from_stdin(self) -> bool:
        return bool(self.nconfig.infiles == ['-'])




class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_gbytes',
                                 type=float)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self,
                      override_filename=None) -> None:

        self.generate_csv_dialect_config(override_filename)
        self.generate_csv_header_config(override_filename)


    def validate_custom_config(self, config) -> None:

        # At this point infiles could be either a default string or a list:
        assert isinstance(config['infiles'], list)



class TooMuchDataError(Exception):
    pass


if __name__ == '__main__':
    sys.exit(main())
