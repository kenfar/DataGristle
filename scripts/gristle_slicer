#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is ':' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is ':' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.

    --max-mem-recs RECS The total number of recs to keep in memory.  If the number of
                        records in the file is greater than this it will use a slower
                        process.

Notes:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset'
    This specification is either a comma-delimited list of individual offsets or ranges.
    Offsets are based on zero, and if negative are measured from the end of the record
    or file with -1 being the final item.  There can be N number of individual offsets.
    Ranges are a pair of offsets separated by a colon.  The first number indicates the
    starting offset, and the second number indicates the stop offset +1.

Python's indexing rules for a single offset and how gristle_slicer differs:
    1. The offset is based on 0
    2. Negative offsets are based on the end of the list
    3. A positive offset greater than the length of the list results in IndexError
    4. A negative offset that references beyond the front of the list - IndexError

Python's slicing rules for a range and how gristle_slicer differs:
    1. The first number is the inclusive start, the second the exclusive end
    2. The ending offset defaults to length of the list
    3. The starting offset defaults to 0
    4. The ending offset can be longer than the list length - but nothing happens
    5. An impossible range produces an empty string, ex: s[10:2] == ''
    6. the step cannot be 0
    7. combining negs & poss can work: s[-3:9] or s[1:-1]is fine
    8. the starting offset can wrap *once*, ex: s[-4:4] == s[-99999:4]

Python's extended slicing rules and how gristle_slicer differs:
    1. The third item (ex: '2:4:1') is the step, and it describes the increment between
       the start & stop.
    2. The step defaults to 1 - which indicates every record.
    3. Other numbers cause it to step skip records.  Ex: 2:4:2 extracts every 2nd item.
    4. Negative steps cause it to go in reverse.  When using negative steps, the start
       should be the larger number than the stop.  Ex:  4:2:-1

s = [a,b,c,d]
pp(s[::]   == 'dcba'

s = [a,b,c,d]
pp(s[5::-1]   == 'dcba'
pp(s[5:0:-1]  == 'cba'
pp(s[5:-1:-1] == ''         # beyond the right border
pp(s[0:-3:-1] == ''         # wrong order
pp(s[0:-4:-1] == ''         # wrong order
pp(s[0:-5:-1] == 'a'
pp(s[0:-6:-1] == 'a'




{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2021 Ken Farmer
"""
import csv
import errno
import fileinput
import os
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
import time
from typing import List, Tuple, Dict, Any, Optional, IO

import psutil

import datagristle.common as comm
import datagristle.configulator as conf
import datagristle.csvhelper as csvhelper
import datagristle.file_io as file_io
import datagristle.helpdoc as helpdoc
import datagristle.location_slicer as slicer

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)
comm.validate_python_version()
MAX_MEM_REC_CNT = 5000000



def main() -> int:

    start_time = time.time()
    try:
        config_manager = ConfigManager(NAME, SHORT_HELP, LONG_HELP)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA
    pp(f'--------> config & input_handler duration: {time.time() - start_time:.2f}')

    slice_runner = SliceRunner(nconfig)

    slice_runner.prep_files()
    slice_runner.prep_counts()
    slice_runner.prep_post_count()
    slice_runner.process_data()
    slice_runner.close_files()


    return 0




class SliceRunner:

    def __init__(self,
                 nconfig):

        # Avoid getting a rec count unless necessary for negative specs, since
        # it slows down processing of large files:
        #start_time = time.time()

        self.nconfig = nconfig
        self.anyorder = False
        self.input_handler = None
        self.output_handler = None

        self.rec_cnt = -1
        self.col_cnt = -1
        self.rec_specs = None
        self.exrec_specs = None
        self.col_specs = None
        self.excol_specs = None

        self.incl_rec_slicer = None
        self.excl_rec_slicer = None
        self.incl_col_slicer = None
        self.excl_col_slicer = None
        self.valid_rec_spec = None
        self.valid_col_spec = None


    def prep_files(self):
        self.input_handler = file_io.InputHandler(self.nconfig.infiles,
                                                  self.nconfig.dialect,
                                                  return_header=True)
        self.output_handler = file_io.OutputHandler(self.nconfig.outfile,
                                                    self.input_handler.dialect)


    def close_files(self):
        self.input_handler.close()
        self.output_handler.close()


    def prep_counts(self):
        start_time = time.time()
        if self.need_rec_count(self.nconfig.records,
                               self.nconfig.exrecords):
            self.rec_cnt = self.get_rec_count(self.nconfig.infiles, self.input_handler.dialect)
        else:
            self.rec_cnt = -1

        self.col_cnt = len(self.nconfig.header.field_names)-1
        pp(f'--------> prep_counts  duration: {time.time() - start_time:.2f}')


    def prep_post_count(self):
        #pp('--------> prep_specs()')
        self.prep_specs()
        #pp('--------> prep_slicers()')
        self.prep_slicers()
        #pp('--------> precalculate_indexes()')
        self.precalculate_indexes()


    def prep_specs(self):
        start_time = time.time()
        self.rec_specs, self.exrec_specs = self.get_rec_specs(self.rec_cnt)
        self.col_specs, self.excol_specs = self.get_col_specs(self.col_cnt)
        pp(f'--------> prep_specs  duration: {time.time() - start_time:.2f}')


    def prep_slicers(self):

        start_time = time.time()
        #pp('------------> rec_specs')
        self.incl_rec_slicer = slicer.SpecProcessor(self.rec_specs)
        #pp('------------> exrec_specs')
        self.excl_rec_slicer = slicer.SpecProcessor(self.exrec_specs)
        #pp('------------> col_specs')
        self.incl_col_slicer = slicer.SpecProcessor(self.col_specs)
        #pp('------------> excol_specs')
        self.excl_col_slicer = slicer.SpecProcessor(self.excol_specs)

        pp(f'--------> prep_slicers  duration: {time.time() - start_time:.2f}')


    def precalculate_indexes(self):

        start_time = time.time()
        self.merged_rec_spec = []
        count = 0
        if (self.incl_rec_slicer.expanded_specs_valid
        and self.excl_rec_slicer.expanded_specs_valid):
            for spec_item in self.incl_rec_slicer.expanded_specs:
                count += 1
                if spec_item in self.excl_rec_slicer.expanded_specs:
                    continue
                else:
                    self.merged_rec_spec.append(spec_item)

        #pp(f'self.incl_rec_slicer.expanded_specs = {len(self.incl_rec_slicer.expanded_specs)=}, {slicer.get_size(self.incl_rec_slicer.expanded_specs)}')
        #pp(f'{self.merged_rec_spec=} = {len(self.merged_rec_spec)}, {slicer.get_size(self.merged_rec_spec)}')

        self.merged_col_spec = []
        if (self.incl_col_slicer.expanded_specs_valid
        and self.excl_col_slicer.expanded_specs_valid):
            for spec_item in self.incl_col_slicer.expanded_specs:
                if spec_item in self.excl_col_slicer.expanded_specs:
                    continue
                else:
                    self.merged_col_spec.append(spec_item)
        #pp(f'{self.merged_col_spec=} = {len(self.merged_rec_spec)}, {slicer.get_size(self.merged_col_spec)}')

        pp(f'--------> precalculate_indexes  duration: {time.time() - start_time:.2f}')


    def process_data(self):
        start_time = time.time()
        if self.must_process_in_memory():
            self.process_recs_in_memory()
        else:
            self.process_recs_from_file()
        pp(f'--------> process_data duration: {time.time() - start_time:.2f}')


    def must_process_in_memory(self):
        # Note that negatives don't really require processing in memory - we could
        # just first count all records, and then process a second time - unless input is stdin.

        if (self.nconfig.infiles == ['-']
                and (self.rec_specs.specs_have_negatives()
                     or self.exrec_specs.specs_have_negatives())):
            pp('Must Process in memory #1!!!!!!!!!!!!!!!!!!!')
            return True

        if not self.anyorder:
            if (self.rec_specs.specs_are_out_of_order()
                    or self.exrec_specs.specs_are_out_of_order()):
                pp('Must Process in memory #2!!!!!!!!!!!!!!!!!!!')
                return True

        return False



    def need_rec_count(self,
                       records_str,
                       exrecords_str):

        records = records_str.split(',')
        exrecords = exrecords_str.split(',') if exrecords_str else []

        if slicer.spec_has_negatives(records):
            return True
        if slicer.spec_has_negatives(exrecords):
            return True
        if slicer.spec_has_unbounded_end_range(records):
            # needed because a None should be converted to an actual value
            return True
        if exrecords and slicer.spec_has_unbounded_end_range(exrecords):
            # needed because a None should be converted to an actual value
            return True

        return False


    def get_rec_specs(self,
                      rec_cnt: int):

        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []

        record_specs = slicer.Specifications('incl_rec', records, infile_item_count=rec_cnt)
        exrecord_specs = slicer.Specifications('excl_rec', exrecords, infile_item_count=rec_cnt)
        return record_specs, exrecord_specs


    def get_col_specs(self,
                      col_cnt: Optional[int]):

        records = self.nconfig.columns.split(',')
        exrecords = self.nconfig.excolumns.split(',') if self.nconfig.excolumns else []

        col_specs = slicer.Specifications('incl_col', records, infile_item_count=col_cnt, header=self.nconfig.header)
        excol_specs = slicer.Specifications('excl_col', exrecords, infile_item_count=col_cnt, header=self.nconfig.header)

        return col_specs, excol_specs


    def get_rec_count(self,
                      files: List[str],
                      dialect: csv.Dialect) -> int:
        """ Get record counts for input files.
            - Counts have an offset of 0
        """
        rec_cnt = -1
        if files[0] == '-':
            return rec_cnt
        #for _ in csv.reader(fileinput.input(files), dialect):
        #    rec_cnt += 1
        for fn in files:
            with open(fn, 'r', newline='', encoding='utf-8') as inbuf:
                csv_reader = csv.reader(inbuf, dialect=dialect)
                for _ in csv_reader:
                    rec_cnt += 1
        fileinput.close()
        return rec_cnt



    def process_recs_from_file(self) -> None:
        """ Reads the file one record at a time, compares against the
            specification, and then writes out qualifying records and
            columns.
            Args:
                - input_handler
                - output_handler
                - incl_rec_spec
                - excl_rec_spec
                - merged_rec_spec: simple list of which recs to slice
                - merged_col_spec: simple list of which columns to slice
        """
        pp('----------------- process_recs_from_file ----------------------')
        merged_rec_sub = 0
        if self.merged_rec_spec:
            pp('    --------- merged_rec_spec ---- ')
            pp(self.merged_rec_spec)
        else:
            pp('    ---- specs_evaluator ---- ')

        short_circuit_stop_rec = 999_999_999_999
        if self.merged_rec_spec:
            short_circuit_stop_rec = max(self.merged_rec_spec)

        for rec in self.input_handler:
            #pp('a')
            rec_number = self.input_handler.rec_cnt -1

            if self.merged_rec_spec:
                if rec_number > short_circuit_stop_rec:
                    break
                try:
                    #pp('b')
                    merged_rec_sub = self.merged_rec_spec.index(rec_number, max(merged_rec_sub, 0))
                except ValueError:
                    continue
            else:
                #pp('c')
                if not self.incl_rec_slicer.specs_evaluator(rec_number):
                    continue
                elif self.excl_rec_slicer.specs_evaluator(rec_number):
                    continue

            output_rec = []

            if self.merged_col_spec:
                output_rec = self.get_cols_from_index(rec,
                                                      self.merged_col_spec)
            else:
                output_rec = self.get_cols_from_eval(rec,
                                                     len(rec),
                                                     self.incl_col_slicer,
                                                     self.excl_col_slicer)

            if output_rec:
                self.output_handler.write_rec(output_rec)



    def process_recs_in_memory(self) -> None:

        """ Reads the entire file into memory, then processes one record
            at a time, compares against the specification, and then writes
            out qualifying records and columns.
        """
        self.verify_data_size()
        pp('--------------------- process_recs_in_memory -----------------------------')
        all_rows = []

        short_circuit_stop_rec = 999_999_999_999
        if self.merged_rec_spec:
            short_circuit_stop_rec = max(self.merged_rec_spec)

        start_time = time.time()
        for i, row in enumerate(self.input_handler):
            if i > short_circuit_stop_rec:
                break
            all_rows.append(row)
            if i > 5000000:
                break
            if i % 1000000 == 0:
                print('1 million')
            if i % 100000 == 0:
                print('.', end='')
        pp(f'--------> process_recs_in_mem.a duration: {time.time() - start_time:.2f}')
        #all_rows = list(self.input_handler)

        # Consider getting rec count and rebuilding specs
        if self.rec_cnt == -1:
            self.rec_cnt = len(all_rows)
            self.prep_post_count()

        #if self.merged_col_spec:
            #pp('---- merged_col_spec ----')
        #else:
            #pp('---- eval ----')

        col_count = len(all_rows[0])
        #pp(self.merged_rec_spec)
        assert self.merged_rec_spec
        for rec_num in self.merged_rec_spec:
            #pp(f'{rec_num=}')
            output_rec = []
            try:
                if self.merged_col_spec:
                    output_rec = self.get_cols_from_index(all_rows[rec_num],
                                                          self.merged_col_spec)
                else:
                    output_rec = self.get_cols_from_eval(all_rows[rec_num],
                                                         col_count,
                                                         self.incl_col_slicer,
                                                         self.excl_col_slicer)
            except IndexError:
                pass
            if output_rec:
                self.output_handler.write_rec(output_rec)


    def verify_data_size(self):
        if self.rec_cnt > MAX_MEM_REC_CNT:
            comm.abort(f'File is too large for in-memory processing (> {MAX_MEM_REC_CNT} recs)',
                       'But in-mem is required due to out of order recs'
                       ' or negative offsets.  Consider removing negative offsets '
                       ' or providing the --anyorder option')

        tot_size_mb = int(sum([os.path.getsize(fn)
                               for fn in self.nconfig.infiles
                               if fn != '-'])/1024/1024)
        if tot_size_mb > 600:
            comm.abort('File is too large for in-memory processing (> 600 MB)',
                       'But in-mem is required due to out of order recs'
                       ' or negative offsets.  Consider removing negative offsets '
                       ' or providing the --anyorder option')




    def get_cols_from_index(self,
                            input_rec,
                            merged_col_spec):
        output_rec = []
        for col_number in merged_col_spec:
            try:
                output_rec.append(input_rec[col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols
        return output_rec



    def get_cols_from_eval(self,
                           input_rec,
                           col_count,
                           incl_col_slicer,
                           excl_col_slicer):
        output_rec = []
        for col_number in range(0, col_count):
            if incl_col_slicer.specs_evaluator(col_number):
                if not excl_col_slicer.specs_evaluator(col_number):
                    output_rec.append(input_rec[col_number])
        return output_rec




class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_recs',
                                 default=10000,
                                 type=int)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self) -> None:

        self.generate_csv_dialect_config()
        self.generate_csv_header_config()


    def validate_custom_config(self, config) -> None:

        # At this point infiles could be either a default string or a list:
        assert isinstance(config['infiles'], list)

        if config['infiles'] == ['-']:
            if ('-' in config['records']
                    or '-' in config['exrecords']):
                comm.abort('Error: negative values not supported with stdin')



class TooMuchDataError(Exception):
    pass


if __name__ == '__main__':
    sys.exit(main())
