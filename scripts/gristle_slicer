#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is '::1' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is '::1' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.
    --max-mem-gbytes GBYTES
                        The total number of gbytes to use if we process-in-mem.  This is
                        used whenever we have specs with reverse steps, out of order recs,
                        or repeat recs. The default is to use up to 50% of your total
                        memory.

How inclusion (-r, -c) and exclusion (-R, -C) specs are applied:
    * Gristle_slicer first applies the inclusion specs, and if none are provided
      by the user then it defaults to all data.  Then it applies the exclusions.
    * For example -r 10:90, -R 30:40 would first include records 10 thru 89 (based
      on a zero offset), and then it would remove  records 30 thru 39.

How specs (-r, -R, -c, -C) are packaged:
    * Gristle_slicer uses the python slicing syntax for the specs.  But that syntax
      only applies to a single spec.  The gristle_slicer options permit multiple
      specs, for example:
         * -r '3,7,8,30:90,-10:' - includes 5 specs: 3 individual offsets, and two ranges

On the ordering of Specs:
    * Gristle_slicer writes the data in the order of the specs.  So, a spec of
      '5,9,2' will output the records or columns in that order.
    * Out of order records or repeating records (ex: -r 5,5,5) require gristle_slicer
      to first read the data into memory before processing.

How individual specs work:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset:Step'

    Basically, offsets are based on zero, and if negative are measured from the end of
    the record or file with -1 being the final item.  There can be N number of
    individual offsets or ranges.  Ranges are a pair of offsets separated by a colon.
    The first number indicates the starting offset, and the second number indicates
    the stop offset +1.

    Steps allow you to skip records or columns: the default step is 1 - which includes
    every record or column.  A step of 2 will include every other one.  A fractional
    step is treated as a probability in order to randomly include records or columns.

    The only departures from python syntax are the inclusion of fractional steps,
    and that a reference to a non-existing record or column won't raise an exception,
    it will just not return any corresponding data for those offsets.

    The specifications are a comma-delimited list of individual offsets or ranges -
    and almost perfectly comply with python's slicing notation, documented here:
        - https://www.pythontutorial.net/advanced-python/python-slicing/
        - https://www.digitalocean.com/community/tutorials/how-to-index-and-slice-strings-in-python-3


Python's indexing rules for a single offset and how gristle_slicer differs:
    1. The offset is based on 0
    2. Negative offsets are based on the end of the list
    3. Difference: A positive offset greater than the length of the list will
       be ignored
    4. Difference: A negative offset that references beyond the front of the list
       will be ignored

Python's slicing rules for a range and how gristle_slicer differs:
    1. The first number is the inclusive start, the second the exclusive end
    2. The ending offset defaults to length of the list
    3. The starting offset defaults to 0
    4. The ending offset can be longer than the list length - but the difference
       is ignored
    5. When using reverse steps the start position of the range should be larger
       than the stop, for example:  4:2:-1 would write record 2 and then 3.

Python's range step rules and how gristle_slicer differs:
    1. The range step defaults to 1 - which will include every record or colum
    2. Only inclusion specs (record or column may have range steps other than 1
    3. Negative range steps cause the data to be written in reverse order
    4. Range steps less than -1 or greater than 1 cause records or cols to be skipped
    5. Fractional range steps (ex: 0.25) result in a sampling of items within the
       range - for example 0.75 would include approximately 75% of the items.

Known issues and other considerations:
    * Python's argparsing module can be thrown off by a leading dash with a following
      colon, ex: -r -3: is a valid spec.  To get around this just quote it and leave
      a leading space, ex: -r ' -3:'
    * Very large files that have repeating, out of order, or reverse records can
      require a lot of memory.  If gristle_slicer runs out of memory consider allowing
      it more (see: --max_mem_gybes), eliminating these features if unnecessary, or
      splitting the work into multiple steps - to first reduce the size of the file.
    * Streaming from stdin may require the data to be first written to a temp file,
      and then read from the file - if the specifications include negative offsets.

{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2022 Ken Farmer
"""
import csv
import errno
import functools
import os
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
import tempfile
import time
from typing import List, Tuple, Dict, Any, Optional, IO, Hashable

import datagristle.common as comm
import datagristle.configulator as conf
from datagristle import file_io
from datagristle import helpdoc
import datagristle.slice_specs as slicer

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)

MAX_MEM_INDEX_CNT = 20_000_000

comm.validate_python_version()


def main() -> int:

    try:
        config_manager = ConfigManager(NAME,
                                       SHORT_HELP,
                                       LONG_HELP,
                                       validate_dialect=False)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA

    slice_runner = SliceRunner(nconfig)
    slice_runner.config_manager = config_manager
    slice_runner.setup_stage1()
    slice_runner.setup_stage2()
    slice_runner.process_data()
    slice_runner.shutdown()

    return 0



class SliceRunner:

    def __init__(self,
                 nconfig) -> None:

        self.nconfig = nconfig
        self.anyorder = False
        self.input_handler: file_io.InputHandler
        self.output_handler: file_io.OutputHandler
        self.temp_fn = None

        self.rec_cnt = None
        self.col_cnt = None
        self.rec_specs: slicer.Specifications
        self.exrec_specs: slicer.Specifications
        self.col_specs: slicer.Specifications
        self.excol_specs: slicer.Specifications
        self.col_default_range = False

        self.incl_rec_slicer: slicer.SpecProcessor
        self.excl_rec_slicer: slicer.SpecProcessor
        self.incl_col_slicer: slicer.SpecProcessor
        self.excl_col_slicer: slicer.SpecProcessor
        self.valid_rec_spec = None
        self.valid_col_spec = None

        self.is_optimized_with_rec_index = False
        self.is_optimized_with_col_index = False
        self.rec_index_optimization_stop_rec = 0

        self.verify_data_recs = []
        self.mem_limiter = comm.MemoryLimiter(max_mem_gbytes=nconfig.max_mem_gbytes)


    def setup_stage1(self) -> None:
        self._setup_files()


    def setup_stage2(self) -> None:
        try:
            self._setup_specs()
        except (slicer.NegativeOffsetWithoutItemCountError,
                slicer.NegativeStepWithoutItemCountError,
                slicer.UnboundedStopWithoutItemCountError) as err:
            if self.are_infiles_from_stdin():
                self._write_stdin_to_file()
                self.nconfig, _ = self.config_manager.get_config(self.temp_fn)
            self._setup_counts()
            try:
                self._setup_specs()
            except (slicer.NegativeOffsetWithoutItemCountError,
                    slicer.NegativeStepWithoutItemCountError,
                    slicer.UnboundedStopWithoutItemCountError):
                comm.abort('Error: unable to count rows in file to resolve config references!',
                        f'Record count: {self.rec_cnt}, Column count: {self.col_cnt}',
                        verbosity='debug')

        self._setup_slicers()
        self._setup_index_optimization()

        self._pp(' ')
        self._pp('Stage2 Optimizations: ')
        self._pp(f'    is_optimized_for_all_recs: {self.is_optimized_for_all_recs()}')
        self._pp(f'    is_optimized_with_rec_index: {self.is_optimized_with_rec_index}')
        self._pp(f'    is_optimized_for_all_cols: {self.is_optimized_for_all_cols()}')
        self._pp(f'    is_optimized_with_col_index: {self.is_optimized_with_col_index}')
        self._pp(f'    rec_index_optimization_stop_rec: {self.rec_index_optimization_stop_rec}')


    def _write_stdin_to_file(self):
        start_time = time.time()

        assert self.nconfig.infiles[0] == '-'
        _, self.temp_fn = tempfile.mkstemp(prefix='gristle_slicer_stdin_temp_')
        with open(self.temp_fn, 'w', newline='', encoding='utf-8') as outbuf:
            writer = csv.writer(outbuf)
            writer.writerows(self.input_handler)
        self.input_handler = file_io.InputHandler([self.temp_fn],
                                                  self.nconfig.dialect,
                                                  return_header=True)

        self._pp(f'--------> write_stdin_to_file duration: {time.time() - start_time:.2f}')


    def _pp(self,
            val: Any) -> None:
        if self.nconfig.verbosity == 'debug':
            print(val)


    def shutdown(self) -> None:
        self.input_handler.close()
        self.output_handler.close()

        if self.temp_fn:
            os.remove(self.temp_fn)

        file_io.remove_all_temp_files(prefix='gristle_slicer_stdin_temp', min_age_hours=1)


    def _setup_files(self) -> None:
        self.input_handler = file_io.InputHandler(self.nconfig.infiles,
                                                  self.nconfig.dialect,
                                                  return_header=True)
        self.output_handler = file_io.OutputHandler(self.nconfig.outfile,
                                                    self.input_handler.dialect)


    def _setup_counts(self) -> None:
        start_time = time.time()
        if self.temp_fn:
            self.rec_cnt = file_io.get_rec_count([self.temp_fn], self.input_handler.dialect)
        else:
            self.rec_cnt = file_io.get_rec_count(self.nconfig.infiles, self.input_handler.dialect)

        self.col_cnt = len(self.nconfig.header.field_names)
        if self.col_cnt < 0:
            self.col_cnt = None
        if self.nconfig.verbosity == 'debug':
            print(f'--------> setup_counts  duration: {time.time() - start_time:.2f}')



    def _setup_specs(self) -> None:
        """ Will get run multiple times - as more info trickles in!
        """

        # record specs:
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []
        self.rec_specs = slicer.Specifications('incl_rec',
                                                records,
                                                infile_item_count=self.rec_cnt)
        self.exrec_specs = slicer.Specifications('excl_rec',
                                                 exrecords,
                                                 infile_item_count=self.rec_cnt)

        # col specs:
        records = self.nconfig.columns.split(',')
        exrecords = self.nconfig.excolumns.split(',') if self.nconfig.excolumns else []
        self.col_specs = slicer.Specifications('incl_col',
                                                records,
                                                infile_item_count=self.col_cnt,
                                                header=self.nconfig.header)
        self.excol_specs = slicer.Specifications('excl_col',
                                                 exrecords,
                                                 infile_item_count=self.col_cnt,
                                                 header=self.nconfig.header)


    def _setup_slicers(self) -> None:
        self.incl_rec_slicer = slicer.SpecProcessor(self.rec_specs)
        self.excl_rec_slicer = slicer.SpecProcessor(self.exrec_specs)
        self.incl_col_slicer = slicer.SpecProcessor(self.col_specs)
        self.excl_col_slicer = slicer.SpecProcessor(self.excol_specs)


    def is_optimized_for_all_recs(self) -> bool:
        return bool(self.incl_rec_slicer.has_all_inclusions is True
                    and self.excl_rec_slicer.has_exclusions is False)

    def is_optimized_for_all_cols(self) -> bool:
        return bool(self.incl_col_slicer.has_all_inclusions is True
                    and self.excl_col_slicer.has_exclusions is False)


    def _setup_index_optimization(self) -> None:

        start_time = time.time()

        self.rec_index: List[int] = []
        if (self.incl_rec_slicer.indexer.valid
        and self.excl_rec_slicer.indexer.valid
        and len(self.excl_rec_slicer.index) <= 10_000):
        # Pulling this out until we can figure out how to process in mem in reverse order without index
        #and self.is_optimized_for_all_recs() is False
            for spec_item in self.incl_rec_slicer.index:
                if spec_item in self.excl_rec_slicer.index:
                    continue
                else:
                    if len(self.rec_index) > MAX_MEM_INDEX_CNT:
                        self.rec_index = []
                        break
                    self.rec_index.append(spec_item)
            else:
                self.rec_index_optimization_stop_rec = max(self.rec_index, default=0)
                self.is_optimized_with_rec_index = True

        self.col_index: List[int] = []
        if (self.incl_col_slicer.indexer.valid
        and self.excl_col_slicer.indexer.valid
        and self.is_optimized_for_all_cols() is False):
            for spec_item in self.incl_col_slicer.index:
                if spec_item in self.excl_col_slicer.index:
                    continue
                else:
                    if len(self.col_index) > MAX_MEM_INDEX_CNT:
                        self.col_index = []
                        break
                    self.col_index.append(spec_item)
            else:
                self.is_optimized_with_col_index = True
        self.col_default_range = self.incl_col_slicer.indexer.col_default_range

        self._pp(' ')
        self._pp('Index Optimizations: ')
        self._pp(f'    {self.incl_rec_slicer.has_all_inclusions=}')
        self._pp(f'    {self.incl_rec_slicer.indexer.valid=}')
        self._pp(f'    {len(self.incl_rec_slicer.index)=}')
        self._pp(f'    {self.incl_rec_slicer.includes_out_of_order=}')
        self._pp(f'    {self.incl_rec_slicer.includes_repeats=}')
        self._pp(f'    {self.incl_rec_slicer.includes_reverse=}')
        self._pp(' ')
        self._pp(f'    {self.excl_rec_slicer.has_all_inclusions=}')
        self._pp(f'    {self.excl_rec_slicer.indexer.valid=}')
        self._pp(f'    {len(self.excl_rec_slicer.index)=}')
        self._pp(' ')
        self._pp(f'    {self.incl_col_slicer.indexer.valid=}')
        self._pp(f'    {self.excl_col_slicer.indexer.valid=}')
        self._pp(f'--------> setup_index_optimization  duration: {time.time() - start_time:.2f}')


    def prune_col_index(self,
                        actual_col_cnt: int):
        new_index = []
        new_index = [x for x in self.col_index if x <= actual_col_cnt]
        self.col_index = new_index
        self.col_default_range = False


    def process_data(self) -> None:
        start_time = time.time()
        if self.must_process_in_memory():
            self.process_recs_in_memory()
        else:
            self.process_recs_from_file()
        self._pp(f'--------> process_data duration: {time.time() - start_time:.2f}')


    def must_process_in_memory(self) -> bool:

        if (self.incl_rec_slicer.includes_out_of_order
                or self.incl_rec_slicer.includes_repeats
                or self.incl_rec_slicer.includes_reverse):

            if self.is_optimized_with_rec_index:
                return True
            elif self.is_optimized_for_all_recs():
                return True
            else:
                comm.abort('Error: There are out of order, reverse, or repeating rec specs but cannot fit into memory!',
                verbosity='debug')

        return False



    def process_recs_from_file(self) -> None:
        """ Reads the file one record at a time, compares against the
            specification, and then writes out qualifying records and
            columns.
            Args:
                - input_handler
                - output_handler
                - incl_rec_spec
                - excl_rec_spec
                - merged_rec_spec: simple list of which recs to slice
                - merged_col_spec: simple list of which columns to slice
        """
        self._pp(f'process: process_recs_from_file')
        next_index_sub = 0

        for rec_number, rec in enumerate(self.input_handler):

            if self.is_optimized_for_all_recs():
                pass
            elif self.is_optimized_with_rec_index:
                if rec_number > self.rec_index_optimization_stop_rec:
                    if self.are_infiles_from_stdin():
                        # Need to finish reading from file rather than break so that we don't
                        # break pipe for pgm piping data to us.  We'll spin thru the rest of 
                        # the recs as quickly as possible, doing it all right here is about 20%
                        # faster than going thru the main loop:
                        break
                        for _ in self.input_handler:
                            pass
                        break
                    else:
                        break

                try:
                    if rec_number == self.rec_index[next_index_sub]:
                        next_index_sub += 1
                    else:
                        continue
                except IndexError:
                    continue

            else:
                if not self.incl_rec_slicer.specs_evaluator(rec_number):
                    continue
                elif self.excl_rec_slicer.specs_evaluator(rec_number):
                    continue

            output_rec = []

            if self.is_optimized_for_all_cols():
                output_rec = rec
            elif self.is_optimized_with_col_index:
                output_rec = self.get_cols_from_index(rec,
                                                      self.col_index)
                if self.col_default_range:
                    self.prune_col_index(actual_col_cnt=len(rec))
            else:
                output_rec = self.get_cols_from_eval(rec,
                                                     len(rec),
                                                     self.incl_col_slicer,
                                                     self.excl_col_slicer)

            if output_rec:
                self.output_handler.write_rec(record=output_rec)



    def process_recs_in_memory(self) -> None:
        """ Reads the entire file into memory, then processes one record
            at a time, compares against the specification, and then writes
            out qualifying records and columns.
        """
        self._pp(f'process: process_recs_in_memory')

        if self.incl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'incl_rec.indexer.invalid')
        if self.excl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'excl_rec.indexer.invalid')

        all_rows = []

        rec: List[str] = []
        for rec_number, rec in enumerate(self.input_handler):
            if rec_number > self.rec_index_optimization_stop_rec:
                if self.are_infiles_from_stdin():
                    for _ in self.input_handler:
                        pass
                    break
                else:
                    break
            all_rows.append(rec)
            try:
                self.mem_limiter.check_record(rec, rec_number)
            except MemoryError:
                comm.abort('ERROR: too many rows to fit into memory',
                            'Change options to either process_by_file or break the process into multiple steps')

        # if we originally couldn't get col counts because of stdin,
        # update now that we have it all loaded into a list:
        if self.col_cnt in (None, -1):
            self.col_cnt = len(all_rows[0]) - 1
        assert self.col_cnt > -1

        for rec_num in self.rec_index:
            output_rec = []
            try:
                if self.is_optimized_for_all_cols():
                    output_rec = all_rows[rec_num]
                elif self.is_optimized_with_col_index:
                    output_rec = self.get_cols_from_index(all_rows[rec_num],
                                                          self.col_index)
                else:
                    # The slower fallback eval solution:
                    output_rec = self.get_cols_from_eval(all_rows[rec_num],
                                                         self.col_cnt,
                                                         self.incl_col_slicer,
                                                         self.excl_col_slicer)
            except IndexError:
                pass
            if output_rec:
                self.output_handler.write_rec(record=output_rec)



    def get_cols_from_index(self,
                            input_rec: List[str],
                            col_index: List[int]) -> List[str]:
        """ Slightly faster solution (about 20% faster) than evals, but requires
            more memory.
        """
        output_rec = []
        for col_number in col_index:
            try:
                output_rec.append(input_rec[col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols
        return output_rec


    def get_cols_from_eval(self,
                           input_rec: List[str],
                           col_count: int,
                           incl_col_slicer: slicer.SpecProcessor,
                           excl_col_slicer: slicer.SpecProcessor) -> List[str]:
        """ Primarily used for unbounded col ranges with stdin
            About 20% slower than get_cols_from_index()
            WARNING: is this actually safe?  does it process in the right order?
        """
        #fixme: do we need to ensure that this is only used with cols in order?!?!?!?!
        output_rec = []
        for col_number in range(0, col_count):
            if self.cached_col_eval(col_number, incl_col_slicer, excl_col_slicer):
                output_rec.append(input_rec[col_number])
        return output_rec


    @functools.lru_cache
    def cached_col_eval(self,
                        col_number: int,
                        incl_col_slicer,
                        excl_col_slicer) -> bool:
        """ The caching on this eval speeds it up 50-75% on large files
            with many columns
        """
        if incl_col_slicer.specs_evaluator(location=col_number):
            if not excl_col_slicer.specs_evaluator(location=col_number):
                return True
        return False


    def are_infiles_from_stdin(self) -> bool:
        return bool(self.nconfig.infiles == ['-'])




class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_gbytes',
                                 type=float)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self,
                      override_filename=None) -> None:

        self.generate_csv_dialect_config(override_filename)
        self.generate_csv_header_config(override_filename)


    def validate_custom_config(self, config) -> None:

        # At this point infiles could be either a default string or a list:
        assert isinstance(config['infiles'], list)



class TooMuchDataError(Exception):
    pass


if __name__ == '__main__':
    sys.exit(main())
