#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is ':' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is ':' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.

    --max-mem-recs RECS The total number of recs to keep in memory.  If the number of
                        records in the file is greater than this it will use a slower
                        process.

Notes:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset'
    This specification is either a comma-delimited list of individual offsets or ranges.
    Offsets are based on zero, and if negative are measured from the end of the record
    or file with -1 being the final item.  There can be N number of individual offsets.
    Ranges are a pair of offsets separated by a colon.  The first number indicates the
    starting offset, and the second number indicates the stop offset +1.

Python's indexing rules for a single offset and how gristle_slicer differs:
    1. The offset is based on 0
    2. Negative offsets are based on the end of the list
    3. A positive offset greater than the length of the list results in IndexError
    4. A negative offset that references beyond the front of the list - IndexError

Python's slicing rules for a range and how gristle_slicer differs:
    1. The first number is the inclusive start, the second the exclusive end
    2. The ending offset defaults to length of the list
    3. The starting offset defaults to 0
    4. The ending offset can be longer than the list length - but nothing happens
    5. An impossible range produces an empty string, ex: s[10:2] == ''
    6. the step cannot be 0
    7. combining negs & poss can work: s[-3:9] or s[1:-1]is fine
    8. the starting offset can wrap *once*, ex: s[-4:4] == s[-99999:4]

Python's extended slicing rules and how gristle_slicer differs:
    1. The third item (ex: '2:4:1') is the step, and it describes the increment between
       the start & stop.
    2. The step defaults to 1 - which indicates every record.
    3. Other numbers cause it to step skip records.  Ex: 2:4:2 extracts every 2nd item.
    4. Negative steps cause it to go in reverse.  When using negative steps, the start
       should be the larger number than the stop.  Ex:  4:2:-1

s = [a,b,c,d]
pp(s[::]   == 'dcba'

s = [a,b,c,d]
pp(s[5::-1]   == 'dcba'
pp(s[5:0:-1]  == 'cba'
pp(s[5:-1:-1] == ''         # beyond the right border
pp(s[0:-3:-1] == ''         # wrong order
pp(s[0:-4:-1] == ''         # wrong order
pp(s[0:-5:-1] == 'a'
pp(s[0:-6:-1] == 'a'




{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2021 Ken Farmer
"""
import csv
import errno
import fileinput
import functools
import os
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
import time
from typing import List, Tuple, Dict, Any, Optional, IO

import datagristle.common as comm
import datagristle.configulator as conf
import datagristle.csvhelper as csvhelper
import datagristle.file_io as file_io
import datagristle.helpdoc as helpdoc
import datagristle.location_slicer as slicer

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)
comm.validate_python_version()
MAX_MEM_REC_CNT = 5000000



def main() -> int:

    start_time = time.time()
    try:
        config_manager = ConfigManager(NAME, SHORT_HELP, LONG_HELP)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA

    slice_runner = SliceRunner(nconfig)
    slice_runner.setup_stage1()
    slice_runner.setup_stage2()
    slice_runner.process_data()
    slice_runner.shutdown()

    return 0



class SliceRunner:

    def __init__(self,
                 nconfig):

        self.nconfig = nconfig
        self.anyorder = False
        self.input_handler = None
        self.output_handler = None

        self.estimated_rec_cnt = -1
        self.rec_cnt = -1
        self.col_cnt = -1
        self.rec_specs = None
        self.exrec_specs = None
        self.col_specs = None
        self.excol_specs = None

        self.incl_rec_slicer = None
        self.excl_rec_slicer = None
        self.incl_col_slicer = None
        self.excl_col_slicer = None
        self.valid_rec_spec = None
        self.valid_col_spec = None

        self.is_optimized_with_rec_index = False
        self.is_optimized_with_col_index = False
        self.rec_index_optimization_stop_rec = sys.maxsize


    def setup_stage1(self):
        self._setup_files()
        #self._setup_specs()
        self._setup_counts()


    def setup_stage2(self):
        # Gets run twice:
        #    1. initially, right after setup_stage1
        #    2. later, in memory process, if record counts are needed
        self._setup_specs()
        self._setup_slicers()
        self._setup_index_optimization()

        if self.nconfig.verbosity == 'debug':
            print(f'is_optimized_for_all_recs: {self.is_optimized_for_all_recs()}')
            print(f'is_optimized_with_rec_index: {self.is_optimized_with_rec_index}')
            print(f'is_optimized_for_all_cols: {self.is_optimized_for_all_cols()}')
            print(f'is_optimized_with_col_index: {self.is_optimized_with_col_index}')
            print(f'rec_index_optimization_stop_rec: {self.rec_index_optimization_stop_rec}')



    def shutdown(self):
        self.input_handler.close()
        self.output_handler.close()


    def _setup_files(self):
        self.input_handler = file_io.InputHandler(self.nconfig.infiles,
                                                  self.nconfig.dialect,
                                                  return_header=True)
        self.output_handler = file_io.OutputHandler(self.nconfig.outfile,
                                                    self.input_handler.dialect)


    def _setup_counts(self):
        start_time = time.time()
        self.estimated_rec_cnt = file_io.get_approx_rec_count(self.nconfig.infiles)
        if self._is_rec_count_needed():
            self.rec_cnt = file_io.get_rec_count(self.nconfig.infiles, self.input_handler.dialect)
        else:
            self.rec_cnt = -1

        self.col_cnt = len(self.nconfig.header.field_names)-1
        #pp(f'--------> setup_counts  duration: {time.time() - start_time:.2f}')

    def _is_rec_count_needed(self):
        """ We want to avoid getting a rec-count if we can avoid it, since it adds a lot of time
            for large files - due to having to perform csv parsing on each row.

            Also, if we're just going to later on load into memory, then we can do it much faster
            then and rebuild our specs if necessary.
        """
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []

        # Negatives need a count to be translated to positives for the indexes
        if (slicer.spec_has_negatives(records)
                or slicer.spec_has_negatives(exrecords)):
            return True

        # Unbounded ends need a count to be translated to actual offsets for the indexes
        # fixme: there's some cases where we probably don't need counts:
        #   1. if we decide to use eval instead of index
        #   2. if we have incl specs and our excl specs has an unbounded range - beyond the incl spec
        #      - in this case we could just ignore the excl range beyond the incl last item
        #   3. if we can count after we load into memory
        if (slicer.spec_has_unbounded_end(records)
                or exrecords and slicer.spec_has_unbounded_end(exrecords)):
            return True

        return False


    def _setup_specs(self):
        """ Will get run multiple times - as more info trickles in!
        """

        # record specs:
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []
        try:
            self.rec_specs = slicer.Specifications('incl_rec',
                                                   records,
                                                   infile_item_count=self.rec_cnt)
            self.exrec_specs = slicer.Specifications('excl_rec',
                                                     exrecords,
                                                     infile_item_count=self.rec_cnt)
        except slicer.NegativeOffsetWithoutItemCountError:
            comm.abort('Record Negative Offset - Not supported in this case')
        except slicer.NegativeStepWithoutItemCountError:
            comm.abort('Record Step Not supported - Not supported in this case')


        # col specs:
        records = self.nconfig.columns.split(',')
        exrecords = self.nconfig.excolumns.split(',') if self.nconfig.excolumns else []
        try:
            self.col_specs = slicer.Specifications('incl_col',
                                                   records,
                                                   infile_item_count=self.col_cnt,
                                                   header=self.nconfig.header,
                                                   max_items=10_000)
            self.excol_specs = slicer.Specifications('excl_col',
                                                     exrecords,
                                                     infile_item_count=self.col_cnt,
                                                     header=self.nconfig.header,
                                                     max_items=10_000)
        except slicer.NegativeOffsetWithoutItemCountError:
            comm.abort('Col NegativeOffset Not supported!')
        except slicer.NegativeStepWithoutItemCountError:
            comm.abort('Record Step Not supported - Not supported in this case')


    def _setup_slicers(self):

        start_time = time.time()
        self.incl_rec_slicer = slicer.SpecProcessor(self.rec_specs)
        self.excl_rec_slicer = slicer.SpecProcessor(self.exrec_specs)
        self.incl_col_slicer = slicer.SpecProcessor(self.col_specs)
        self.excl_col_slicer = slicer.SpecProcessor(self.excol_specs)
        #pp(f'--------> setup_slicers  duration: {time.time() - start_time:.2f}')


    def is_optimized_for_all_recs(self):
        if (self.incl_rec_slicer.has_all_inclusions is True
                and self.excl_rec_slicer.has_exclusions is False):
            return True
        else:
            return False

    def is_optimized_for_all_cols(self):
        if (self.incl_col_slicer.has_all_inclusions is True
                and self.excl_col_slicer.has_exclusions is False):
            return True
        else:
            return False


    def _setup_index_optimization(self):

        #fixme: redundant magic numbers
        # this validity check should probably be a method in the module:
        # maybe should also ensure that expanded specs are empty in this case?

        start_time = time.time()

        self.rec_index = []
        if (self.incl_rec_slicer.indexer.valid
        and self.excl_rec_slicer.indexer.valid
        and self.is_optimized_for_all_recs() is False
        and len(self.excl_rec_slicer.index) <= 10000):
            for spec_item in self.incl_rec_slicer.index:
                if spec_item in self.excl_rec_slicer.index:
                    continue
                else:
                    if len(self.rec_index) > 1_000_000:
                        self.rec_index = []
                        break
                    self.rec_index.append(spec_item)
            else:
                self.rec_index_optimization_stop_rec = max(self.rec_index) if self.rec_index else -1
                self.is_optimized_with_rec_index = True

        self.col_index = []
        if (self.incl_col_slicer.indexer.valid
        and self.excl_col_slicer.indexer.valid
        and self.is_optimized_for_all_cols() is False):
            for spec_item in self.incl_col_slicer.index:
                if spec_item in self.excl_col_slicer.index:
                    continue
                else:
                    if len(self.col_index) > 10_000:
                        self.col_index = []
                        break
                    self.col_index.append(spec_item)
            else:
                self.is_optimized_with_col_index = True


        #print(f'{self.incl_rec_slicer.has_all_inclusions=}')
        #print(f'{self.incl_rec_slicer.expanded_specs_valid=}')
        #print(f'{len(self.incl_rec_slicer.expanded_specs)=}')
        #print()
        #print(f'{self.excl_rec_slicer.has_all_inclusions=}')
        #print(f'{self.excl_rec_slicer.expanded_specs_valid=}')
        #print(f'{len(self.excl_rec_slicer.expanded_specs)=}')
        #print()
        print(f'{self.incl_col_slicer.indexer.valid=}')
        print(f'{self.excl_col_slicer.indexer.valid=}')

        #print(f'--------> setup_index_optimization  duration: {time.time() - start_time:.2f}')
        #print('======================================================')


    def process_data(self):
        start_time = time.time()
        if self.must_process_in_memory():
            self.process_recs_in_memory()
        else:
            self.process_recs_from_file()
        #pp(f'--------> process_data duration: {time.time() - start_time:.2f}')


    def must_process_in_memory(self):
        # Note that negatives don't really require processing in memory - we could
        # just first count all records, and then process a second time - unless input is stdin.

        # WARNING: the spec cleaner leaves negatives as-is if the item-count is -1, so that we can later
        # more reprocess after we get the rec-count from loading into memory...but this means that its
        # determination of out of order or repeats could be wrong?

        if (self.incl_rec_slicer.indexer.valid
                    and slicer.is_out_of_order(self.incl_rec_slicer.index)) \
                or (self.excl_rec_slicer.indexer.valid
                    and slicer.is_out_of_order(self.excl_rec_slicer.index)):
            return True

        if self.incl_rec_slicer.indexer.has_repeats():
            return True

        return False



    def process_recs_from_file(self) -> None:
        """ Reads the file one record at a time, compares against the
            specification, and then writes out qualifying records and
            columns.
            Args:
                - input_handler
                - output_handler
                - incl_rec_spec
                - excl_rec_spec
                - merged_rec_spec: simple list of which recs to slice
                - merged_col_spec: simple list of which columns to slice
        """
        pp('----------------- process_recs_from_file ----------------------')
        index_rec_start= 0

        for rec_number, rec in enumerate(self.input_handler):

            if self.is_optimized_for_all_recs():
                pass
            elif self.is_optimized_with_rec_index:
                if rec_number > self.rec_index_optimization_stop_rec:
                    continue
                try:
                    # This only works here because:
                    #  1. the rec spec must not be out of order - it's a requirement of process_recs_from_file
                    #  2. the rec spec must not be reverse order
                    #  3. or the anyorder option was provided
                    #index_rec_start = self.rec_index.index(rec_number, max(index_rec_start, 0))
                    index_rec_start = self.rec_index.index(rec_number, index_rec_start)
                except ValueError as err: # record position not found in merged_rec_spec - must bypass
                    continue
            else:
                if not self.incl_rec_slicer.specs_evaluator(rec_number):
                    continue
                elif self.excl_rec_slicer.specs_evaluator(rec_number):
                    continue

            output_rec = []

            if self.is_optimized_for_all_cols():
                #todo: Consider enhancing file_io.py to allow us to handle pre-csv parsed rec
                output_rec = rec
            elif self.is_optimized_with_col_index:
                output_rec = self.get_cols_from_index(rec,
                                                      self.col_index)
            else:
                output_rec = self.get_cols_from_eval(rec,
                                                     len(rec),
                                                     self.incl_col_slicer,
                                                     self.excl_col_slicer)

            if output_rec:
                self.output_handler.write_rec(output_rec)



    def process_recs_in_memory(self) -> None:

        """ Reads the entire file into memory, then processes one record
            at a time, compares against the specification, and then writes
            out qualifying records and columns.
        """
        if self.incl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'incl_rec.indexe.invalid')
        if self.excl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'excl_rec.indexer.invalid')

        if (self.incl_col_slicer.indexer.valid is False
                or self.excl_col_slicer.indexer.valid is False):
            print('!!! warning: bad spec index!!!')

        self.verify_data_size()
        pp('--------------------- process_recs_in_memory -----------------------------')
        all_rows = []

        start_time = time.time()
        for rec_number, rec in enumerate(self.input_handler):
            if self.rec_cnt > -1 and rec_number > self.rec_index_optimization_stop_rec:
                continue
            all_rows.append(rec)
            if rec_number > 5_000_000:
                break
                #fixme: should raise exception here

        # if we originally couldn't get rec counts or col counts because of stdin,
        # retry now that we have it all loaded into a list:
        if self.col_cnt == -1:
            self.col_cnt = len(all_rows[0]) - 1


        assert self.rec_index
        for rec_num in self.rec_index:
            output_rec = []
            try:
                if self.is_optimized_for_all_cols():
                    #todo: Consider enhancing file_io.py to handle pre-csv parsed rec
                    output_rec = all_rows[rec_num]
                elif self.is_optimized_with_col_index:
                    output_rec = self.get_cols_from_index(all_rows[rec_num],
                                                          self.col_index)
                else:
                    # This option should not ever get run for memory-processing,
                    # but leaving it in for now - in case we find limits with our
                    # ability to rerun setup_post_count()
                    output_rec = self.get_cols_from_eval(all_rows[rec_num],
                                                         self.col_cnt,
                                                         self.incl_col_slicer,
                                                         self.excl_col_slicer)
            except IndexError:
                pass
            if output_rec:
                self.output_handler.write_rec(output_rec)


    def verify_data_size(self):
        if self.rec_cnt > MAX_MEM_REC_CNT:
            comm.abort(f'File is too large for in-memory processing (> {MAX_MEM_REC_CNT} recs)',
                       'But in-mem is required due to out of order recs'
                       ' or negative offsets.  Consider removing negative offsets '
                       ' or providing the --anyorder option')

        if file_io.get_file_size(self.nconfig.infiles) > 600*1024*1024:
            comm.abort('File is too large for in-memory processing (> 600 MB)',
                       'But in-mem is required due to out of order recs'
                       ' or negative offsets.  Consider removing negative offsets '
                       ' or providing the --anyorder option')




    def get_cols_from_index(self,
                            input_rec,
                            merged_col_spec):
        """ Slightly faster solution (about 20% faster) than evals, but requires
            more memory.
        """
        output_rec = []
        for col_number in merged_col_spec:
            try:
                output_rec.append(input_rec[col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols
        return output_rec


    def get_cols_from_eval(self,
                           input_rec,
                           col_count,
                           incl_col_slicer,
                           excl_col_slicer):
        """ Primarily used for unbounded col ranges with stdin
            About 20% slower than get_cols_from_index()
            WARNING: is this actually safe?  does it process in the right order?
        """
        #fixme: do we need to ensure that this is only used with cols in order?!?!?!?!
        output_rec = []
        for col_number in range(0, col_count):
            if self.cached_col_eval(col_number, incl_col_slicer, excl_col_slicer):
                output_rec.append(input_rec[col_number])
        return output_rec


    @functools.lru_cache
    def cached_col_eval(self,
                        col_number,
                        incl_col_slicer,
                        excl_col_slicer):
        """ The caching on this eval speeds it up 50-75% on large files
            with many columns
        """
        if incl_col_slicer.specs_evaluator(col_number):
            if not excl_col_slicer.specs_evaluator(col_number):
                return True
        return False





class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_recs',
                                 default=10000,
                                 type=int)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self) -> None:

        self.generate_csv_dialect_config()
        self.generate_csv_header_config()


    def validate_custom_config(self, config) -> None:

        # At this point infiles could be either a default string or a list:
        assert isinstance(config['infiles'], list)

        if config['infiles'] == ['-']:
            if ('-' in config['records']
                    or '-' in config['exrecords']):
                comm.abort('Error: negative values not supported with stdin')



class TooMuchDataError(Exception):
    pass


if __name__ == '__main__':
    sys.exit(main())
