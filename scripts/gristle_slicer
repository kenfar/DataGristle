#!/usr/bin/env python
"""
Gristle_slicer extracts subsets of input files based on user-specified columns and rows.  The
input csv file can be piped into the program through stdin or identified via a command line option.
The output will default to stdout, or redirected to a filename via a command line option.

The columns and rows are specified using python list slicing syntax - so individual columns or
rows can be listed as can ranges.   Inclusion or exclusion logic can be used - and even combined.

Usage: gristle_slicer [options]


{see: helpdoc.HELP_SECTION}


Main Options:
    -i, --infiles INFILE
                        One or more input files or '-' (the default) for stdin.
    -o, --outfile OUTFILE
                        The output file.  The default of '-' is stdout.
    -c, --columns SPEC  The column inclusion specification.
                        Default is ':' which includes all columns.
    -C, --excolumns SPEC
                        The column exclusion specification.
                        Default is None which excludes nothing.
    -r, --records SPEC  The record inclusion specification.
                        Default is ':' which includes all records.
    -R, --exrecords SPEC
                        The record exclusion specification.
                        Default is None which excludes nothing.

    --max-mem-recs RECS The total number of recs to keep in memory.  If the number of
                        records in the file is greater than this it will use a slower
                        process.

Notes:
    Supported slicing specification - for columns (-c, -C) and rows (-r, -R):
        'NumberN, StartOffset:StopOffset'
    This specification is either a comma-delimited list of individual offsets or ranges.
    Offsets are based on zero, and if negative are measured from the end of the record
    or file with -1 being the final item.  There can be N number of individual offsets.
    Ranges are a pair of offsets separated by a colon.  The first number indicates the
    starting offset, and the second number indicates the stop offset +1.

Python's indexing rules for a single offset and how gristle_slicer differs:
    1. The offset is based on 0
    2. Negative offsets are based on the end of the list
    3. A positive offset greater than the length of the list results in IndexError
    4. A negative offset that references beyond the front of the list - IndexError

Python's slicing rules for a range and how gristle_slicer differs:
    1. The first number is the inclusive start, the second the exclusive end
    2. The ending offset defaults to length of the list
    3. The starting offset defaults to 0
    4. The ending offset can be longer than the list length - but nothing happens
    5. An impossible range produces an empty string, ex: s[10:2] == ''
    6. the step cannot be 0
    7. combining negs & poss can work: s[-3:9] or s[1:-1]is fine
    8. the starting offset can wrap *once*, ex: s[-4:4] == s[-99999:4]

Python's extended slicing rules and how gristle_slicer differs:
    1. The third item (ex: '2:4:1') is the step, and it describes the increment between
       the start & stop.
    2. The step defaults to 1 - which indicates every record.
    3. Other numbers cause it to step skip records.  Ex: 2:4:2 extracts every 2nd item.
    4. Negative steps cause it to go in reverse.  When using negative steps, the start
       should be the larger number than the stop.  Ex:  4:2:-1

s = [a,b,c,d]
pp(s[::]   == 'dcba'

s = [a,b,c,d]
pp(s[5::-1]   == 'dcba'
pp(s[5:0:-1]  == 'cba'
pp(s[5:-1:-1] == ''         # beyond the right border
pp(s[0:-3:-1] == ''         # wrong order
pp(s[0:-4:-1] == ''         # wrong order
pp(s[0:-5:-1] == 'a'
pp(s[0:-6:-1] == 'a'




{see: helpdoc.CSV_SECTION}


{see: helpdoc.CONFIG_SECTION}


Examples:
    $ gristle_slicer -i sample.csv
                            Prints all rows and columns
    $ gristle_slicer -i sample.csv -c":5, 10:15" -C 13
                            Prints columns 0-4 and 10,11,12,14 for all records
    $ gristle_slicer -i sample.csv -C:-1
                            Prints all columns except for the last for all
                            records
    $ gristle_slicer -i sample.csv -c:5 -r-100:
                            Prints columns 0-4 for the last 100 records
    $ gristle_slicer -i sample.csv -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ cat sample.csv | gristle_slicer -c:5 -r-100 -d'|' --quoting=quote_all:
                            Prints columns 0-4 for the last 100 records, csv
                            dialect info (delimiter, quoting) provided manually)
    $ gristle_slicer -i sample.csv -r '-20 : -1'
                            Prints a negative range - note that it must be quoted
                            AND there must be spaces around the colon - otherwise
                            the argument parsing will produce the error:
                            "expected one argument"
    Many more examples can be found here:
        https://github.com/kenfar/DataGristle/tree/master/examples/gristle_slicer


Licensing and Further Info:
    This source code is protected by the BSD license.  See the file "LICENSE"
    in the source code root directory for the full language or refer to it here:
       http://opensource.org/licenses/BSD-3-Clause
    Copyright 2011-2021 Ken Farmer
"""
import csv
import errno
import fileinput
import functools
import os
from os.path import basename
from pprint import pprint as pp
from signal import signal, SIGPIPE, SIG_DFL
import sys
import time
from typing import List, Tuple, Dict, Any, Optional, IO, Hashable

import datagristle.common as comm
import datagristle.configulator as conf
from datagristle import file_io
from datagristle import helpdoc
import datagristle.location_slicer as slicer
import psutil

#Ignore SIG_PIPE and don't throw exceptions on it... (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

NAME = basename(__file__)
LONG_HELP = helpdoc.expand_long_help(__doc__)
SHORT_HELP = helpdoc.get_short_help_from_long(LONG_HELP)

MAX_MEM_REC_CNT = 1_000_000
MAX_MEM_FILE_SIZE_MB = 600

comm.validate_python_version()


def main() -> int:

    try:
        config_manager = ConfigManager(NAME, SHORT_HELP, LONG_HELP)
        nconfig, _ = config_manager.get_config()
    except EOFError:
        return errno.ENODATA

    slice_runner = SliceRunner(nconfig)
    slice_runner.setup_stage1()
    slice_runner.setup_stage2()
    slice_runner.process_data()
    slice_runner.shutdown()

    return 0



class SliceRunner:

    def __init__(self,
                 nconfig) -> None:

        self.nconfig = nconfig
        self.anyorder = False
        self.input_handler: file_io.InputHandler
        self.output_handler: file_io.OutputHandler

        self.estimated_rec_cnt = -1
        self.rec_cnt = -1
        self.col_cnt = -1
        self.rec_specs: slicer.Specifications
        self.exrec_specs: slicer.Specifications
        self.col_specs: slicer.Specifications
        self.excol_specs: slicer.Specifications

        self.incl_rec_slicer: slicer.SpecProcessor
        self.excl_rec_slicer: slicer.SpecProcessor
        self.incl_col_slicer: slicer.SpecProcessor
        self.excl_col_slicer: slicer.SpecProcessor
        self.valid_rec_spec = None
        self.valid_col_spec = None

        self.is_optimized_with_rec_index = False
        self.is_optimized_with_col_index = False
        self.rec_index_optimization_stop_rec = sys.maxsize


    def setup_stage1(self) -> None:
        self._setup_files()
        self._setup_counts()


    def setup_stage2(self) -> None:
        self._setup_specs()
        self._setup_slicers()
        self._setup_index_optimization()

        self._pp(f'is_optimized_for_all_recs: {self.is_optimized_for_all_recs()}')
        self._pp(f'is_optimized_with_rec_index: {self.is_optimized_with_rec_index}')
        self._pp(f'is_optimized_for_all_cols: {self.is_optimized_for_all_cols()}')
        self._pp(f'is_optimized_with_col_index: {self.is_optimized_with_col_index}')
        self._pp(f'rec_index_optimization_stop_rec: {self.rec_index_optimization_stop_rec}')
        assert isinstance(self.rec_index_optimization_stop_rec, int)

    def _pp(self,
            val: Any) -> None:
        if self.nconfig.verbosity == 'debug':
            pp(val)


    def shutdown(self) -> None:
        self.input_handler.close()
        self.output_handler.close()


    def _setup_files(self) -> None:
        self.input_handler = file_io.InputHandler(self.nconfig.infiles,
                                                  self.nconfig.dialect,
                                                  return_header=True)
        self.output_handler = file_io.OutputHandler(self.nconfig.outfile,
                                                    self.input_handler.dialect)


    def _setup_counts(self) -> None:
        start_time = time.time()
        self.estimated_rec_cnt = file_io.get_approx_rec_count(self.nconfig.infiles)
        if self._is_rec_count_needed():
            self.rec_cnt = file_io.get_rec_count(self.nconfig.infiles, self.input_handler.dialect)
        else:
            self.rec_cnt = -1

        self.col_cnt = len(self.nconfig.header.field_names)-1
        if self.nconfig.verbosity == 'debug':
            pp(f'--------> setup_counts  duration: {time.time() - start_time:.2f}')


    def _is_rec_count_needed(self) -> bool:
        """ We want to avoid getting a rec-count if we can avoid it, since it adds a lot of time
            for large files - due to having to perform csv parsing on each row.

            Also, if we're just going to later on load into memory, then we can do it much faster
            then and rebuild our specs if necessary.
        """
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []

        #if slicer.includes_all_or_none(records) and slicer.includes_all_or_none(exrecords):
        #    pp('------------------1111111111111111')
        #    return False

        # Negatives need a count to be translated to positives for the indexes
        if (slicer.spec_has_negatives(records)
                or slicer.spec_has_negatives(exrecords)):
            return True

        # Unbounded ends need a count to be translated to actual offsets for the indexes
        # fixme: there's some cases where we probably don't need counts:
        #   1. if we decide to use eval instead of index
        #   2. if we have incl specs and our excl specs has an unbounded range - beyond the incl spec
        #      - in this case we could just ignore the excl range beyond the incl last item
        #   3. if we can count after we load into memory
        if (slicer.spec_has_unbounded_end(records)
                or exrecords and slicer.spec_has_unbounded_end(exrecords)):
            return True

        return False


    def _setup_specs(self) -> None:
        """ Will get run multiple times - as more info trickles in!
        """

        # record specs:
        records = self.nconfig.records.split(',')
        exrecords = self.nconfig.exrecords.split(',') if self.nconfig.exrecords else []
        try:
            self.rec_specs = slicer.Specifications('incl_rec',
                                                   records,
                                                   infile_item_count=self.rec_cnt,
                                                   max_items=MAX_MEM_REC_CNT)
            self.exrec_specs = slicer.Specifications('excl_rec',
                                                     exrecords,
                                                     infile_item_count=self.rec_cnt,
                                                     max_items=MAX_MEM_REC_CNT)
        except slicer.NegativeOffsetWithoutItemCountError:
            comm.abort('Record Negative Offset - Not supported in this case')
        except slicer.NegativeStepWithoutItemCountError:
            comm.abort('Record Negative Step Not supported - Not supported in this case')


        # col specs:
        records = self.nconfig.columns.split(',')
        exrecords = self.nconfig.excolumns.split(',') if self.nconfig.excolumns else []
        try:
            self.col_specs = slicer.Specifications('incl_col',
                                                   records,
                                                   infile_item_count=self.col_cnt,
                                                   header=self.nconfig.header,
                                                   max_items=10_000)
            self.excol_specs = slicer.Specifications('excl_col',
                                                     exrecords,
                                                     infile_item_count=self.col_cnt,
                                                     header=self.nconfig.header,
                                                     max_items=10_000)
        except slicer.NegativeOffsetWithoutItemCountError:
            comm.abort('Col NegativeOffset Not supported!')
        except slicer.NegativeStepWithoutItemCountError:
            comm.abort('Record Negative Step Not supported - Not supported in this case')


    def _setup_slicers(self) -> None:
        self.incl_rec_slicer = slicer.SpecProcessor(self.rec_specs)
        self.excl_rec_slicer = slicer.SpecProcessor(self.exrec_specs)
        self.incl_col_slicer = slicer.SpecProcessor(self.col_specs)
        self.excl_col_slicer = slicer.SpecProcessor(self.excol_specs)


    def is_optimized_for_all_recs(self) -> bool:
        return bool(self.incl_rec_slicer.has_all_inclusions is True
                    and self.excl_rec_slicer.has_exclusions is False)

    def is_optimized_for_all_cols(self) -> bool:
        return bool(self.incl_col_slicer.has_all_inclusions is True
                    and self.excl_col_slicer.has_exclusions is False)


    def _setup_index_optimization(self) -> None:

        start_time = time.time()

        self.rec_index: List[int] = []
        if (self.incl_rec_slicer.indexer.valid
        and self.excl_rec_slicer.indexer.valid
        and self.is_optimized_for_all_recs() is False
        and len(self.excl_rec_slicer.index) <= 10_000):
            for spec_item in self.incl_rec_slicer.index:
                if spec_item in self.excl_rec_slicer.index:
                    continue
                else:
                    if len(self.rec_index) > 1_000_000:
                        self.rec_index = []
                        break
                    self.rec_index.append(spec_item)
            else:
                self.rec_index_optimization_stop_rec = max(self.rec_index, default=sys.maxsize)
                self.is_optimized_with_rec_index = True

        self.col_index: List[int] = []
        if (self.incl_col_slicer.indexer.valid
        and self.excl_col_slicer.indexer.valid
        and self.is_optimized_for_all_cols() is False):
            for spec_item in self.incl_col_slicer.index:
                if spec_item in self.excl_col_slicer.index:
                    continue
                else:
                    if len(self.col_index) > 10_000:
                        self.col_index = []
                        break
                    self.col_index.append(spec_item)
            else:
                self.is_optimized_with_col_index = True


        self._pp(f'{self.incl_rec_slicer.has_all_inclusions=}')
        self._pp(f'{self.incl_rec_slicer.indexer.valid=}')
        self._pp(f'{len(self.incl_rec_slicer.index)=}')
        self._pp(f'{self.incl_rec_slicer.includes_out_of_order=}')
        self._pp(f'{self.incl_rec_slicer.includes_repeats=}')
        self._pp(f'{self.incl_rec_slicer.includes_reverse=}')


        self._pp(' ')
        self._pp(f'{self.excl_rec_slicer.has_all_inclusions=}')
        self._pp(f'{self.excl_rec_slicer.indexer.valid=}')
        self._pp(f'{len(self.excl_rec_slicer.index)=}')
        self._pp(' ')
        self._pp(f'{self.incl_col_slicer.indexer.valid=}')
        self._pp(f'{self.excl_col_slicer.indexer.valid=}')
        self._pp(f'--------> setup_index_optimization  duration: {time.time() - start_time:.2f}')


    def process_data(self) -> None:
        start_time = time.time()
        if self.must_process_in_memory():
            self.process_recs_in_memory()
        else:
            self.process_recs_from_file()
        self._pp(f'--------> process_data duration: {time.time() - start_time:.2f}')


    def must_process_in_memory(self) -> bool:

        # Problem: the only way we know if its out of order is with the index.  If the index build fails, we don't know!
        if (self.incl_rec_slicer.includes_out_of_order
                or self.incl_rec_slicer.includes_repeats
                or self.incl_rec_slicer.includes_reverse):
            if self.is_optimized_with_rec_index:
                return True
            else:
                comm.abort('Error: There are out of order, reverse, or repeating rec specs but cannot fit into memory!')

        return False



    def process_recs_from_file(self) -> None:
        """ Reads the file one record at a time, compares against the
            specification, and then writes out qualifying records and
            columns.
            Args:
                - input_handler
                - output_handler
                - incl_rec_spec
                - excl_rec_spec
                - merged_rec_spec: simple list of which recs to slice
                - merged_col_spec: simple list of which columns to slice
        """
        self._pp(f'process_recs_from_file')
        index_rec_start= 0

        for rec_number, rec in enumerate(self.input_handler):

            if self.is_optimized_for_all_recs():
                pass
            elif self.is_optimized_with_rec_index:
                if rec_number > self.rec_index_optimization_stop_rec:
                    if self.is_infiles_from_stdin():
                        # spin thru the rest of the recs as quickly as possible, doing it all right
                        # here is about 20% faster than going thru the main loop:
                        for rec in self.input_handler:
                            pass
                        break
                    else:
                        break
                try:
                    # This only works here because:
                    #  1. the rec spec must not be out of order - it's a requirement of process_recs_from_file
                    #  2. the rec spec must not be reverse order
                    #  3. or the anyorder option was provided
                    #index_rec_start = self.rec_index.index(rec_number, max(index_rec_start, 0))
                    index_rec_start = self.rec_index.index(rec_number, index_rec_start)
                except ValueError as err: # record position not found in merged_rec_spec - must bypass
                    continue
            else:
                # fixme: should we also have some kind of stop optimization here?
                if not self.incl_rec_slicer.specs_evaluator(rec_number):
                    continue
                elif self.excl_rec_slicer.specs_evaluator(rec_number):
                    continue

            output_rec = []

            if self.is_optimized_for_all_cols():
                output_rec = rec
            elif self.is_optimized_with_col_index:
                output_rec = self.get_cols_from_index(rec,
                                                      self.col_index)
            else:
                output_rec = self.get_cols_from_eval(rec,
                                                     len(rec),
                                                     self.incl_col_slicer,
                                                     self.excl_col_slicer)

            if output_rec:
                self.output_handler.write_rec(record=output_rec)



    def process_recs_in_memory(self) -> None:
        """ Reads the entire file into memory, then processes one record
            at a time, compares against the specification, and then writes
            out qualifying records and columns.
        """
        self._pp(f'process_recs_in_memory')

        if self.incl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'incl_rec.indexe.invalid')
        if self.excl_rec_slicer.indexer.valid is False:
            comm.abort('Unable to process in memory', 'excl_rec.indexer.invalid')

        self.verify_data_size()
        all_rows = []

        rec: List[str] = []
        for rec_number, rec in enumerate(self.input_handler):
            if rec_number > self.rec_index_optimization_stop_rec:
                if self.is_infiles_from_stdin():
                    for rec in self.input_handler:
                        pass
                    break
                else:
                    break
            all_rows.append(rec)
            if rec_number > MAX_MEM_REC_CNT:
                comm.abort('ERROR:  too many rows to fit into memory',
                           'Change options to either process_by_file or process in steps')

        # if we originally couldn't get col counts because of stdin,
        # update now that we have it all loaded into a list:
        if self.col_cnt == -1:
            self.col_cnt = len(all_rows[0]) - 1

        for rec_num in self.rec_index:
            output_rec = []
            try:
                if self.is_optimized_for_all_cols():
                    output_rec = all_rows[rec_num]
                elif self.is_optimized_with_col_index:
                    output_rec = self.get_cols_from_index(all_rows[rec_num],
                                                          self.col_index)
                else:
                    # The slower fallback eval solution:
                    output_rec = self.get_cols_from_eval(all_rows[rec_num],
                                                         self.col_cnt,
                                                         self.incl_col_slicer,
                                                         self.excl_col_slicer)
            except IndexError:
                pass
            if output_rec:
                self.output_handler.write_rec(record=output_rec)


    def verify_data_size(self) -> None:

        total_mem_mb = psutil.virtual_memory().total/1024/1024
        file_size_mb = int(file_io.get_file_size(self.nconfig.infiles)/1024/1024)
        full_file_mem_required_mb = file_size_mb * 20
        available_mem_mb = total_mem_mb * 0.67
        avg_rec_size = file_io.get_file_avg_rec_size(self.nconfig.infiles,
                                                     self.nconfig.dialect)

        if avg_rec_size is None:
            part_file_mem_required_mb = None
            actual_mem_required_mb = None
        else:
            part_file_mem_required_mb = avg_rec_size * self.rec_index_optimization_stop_rec/1024/1024
            actual_mem_required_mb = min(full_file_mem_required_mb, part_file_mem_required_mb)

        pp('^^^^^^^^^^^^^^^^^^^^^^^^^^')
        pp(total_mem_mb)
        pp(file_size_mb)
        pp(f'{self.rec_index_optimization_stop_rec=}')
        pp(f'{avg_rec_size=}')
        pp(f'{full_file_mem_required_mb=}')
        pp(f'{part_file_mem_required_mb=}')
        pp(f'{available_mem_mb=}')
        pp('^^^^^^^^^^^^^^^^^^^^^^^^^^')

        msg = ('File is too large for in-memory processing (> 600 MB)'
               'But in-mem is required due to out of order recs'
               ' or negative offsets.  Consider removing negative offsets '
               ' or providing the --anyorder option')

        if actual_mem_required_mb is not None and actual_mem_required_mb > available_mem_mb:
            comm.abort(msg)



    def get_cols_from_index(self,
                            input_rec: List[str],
                            col_index: List[int]) -> List[str]:
        """ Slightly faster solution (about 20% faster) than evals, but requires
            more memory.
        """
        output_rec = []
        for col_number in col_index:
            try:
                output_rec.append(input_rec[col_number])
            except IndexError:
                pass # maybe a short record, or user provided a spec that exceeded cols
        return output_rec


    def get_cols_from_eval(self,
                           input_rec: List[str],
                           col_count: int,
                           incl_col_slicer: slicer.SpecProcessor,
                           excl_col_slicer: slicer.SpecProcessor) -> List[str]:
        """ Primarily used for unbounded col ranges with stdin
            About 20% slower than get_cols_from_index()
            WARNING: is this actually safe?  does it process in the right order?
        """
        #fixme: do we need to ensure that this is only used with cols in order?!?!?!?!
        output_rec = []
        for col_number in range(0, col_count):
            if self.cached_col_eval(col_number, incl_col_slicer, excl_col_slicer):
                output_rec.append(input_rec[col_number])
        return output_rec


    @functools.lru_cache
    def cached_col_eval(self,
                        col_number: int,
                        incl_col_slicer,
                        excl_col_slicer) -> bool:
        """ The caching on this eval speeds it up 50-75% on large files
            with many columns
        """
        if incl_col_slicer.specs_evaluator(location=col_number):
            if not excl_col_slicer.specs_evaluator(location=col_number):
                return True
        return False


    def is_infiles_from_stdin(self) -> bool:
        return bool(self.nconfig.infiles == ['-'])




class ConfigManager(conf.Config):


    def define_user_config(self) -> None:
        """ Defines the user config or metadata.

        Does not get the user input.
        """
        self.add_standard_metadata('infiles')
        self.add_standard_metadata('outfile')

        self.add_custom_metadata(name='columns',
                                 short_name='c',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='excolumns',
                                 short_name='C',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='records',
                                 short_name='r',
                                 default=':',
                                 type=str)
        self.add_custom_metadata(name='exrecords',
                                 short_name='R',
                                 default='',
                                 type=str)
        self.add_custom_metadata(name='max_mem_recs',
                                 default=10000,
                                 type=int)

        self.add_standard_metadata('verbosity')
        self.add_all_config_configs()
        self.add_all_csv_configs()
        self.add_all_help_configs()


    def extend_config(self) -> None:

        self.generate_csv_dialect_config()
        self.generate_csv_header_config()


    def validate_custom_config(self, config) -> None:

        # At this point infiles could be either a default string or a list:
        assert isinstance(config['infiles'], list)

        #if config['infiles'] == ['-']:
        #    if ('-' in config['records']
        #            or '-' in config['exrecords']):
        #        comm.abort('Error: negative values not supported with stdin')



class TooMuchDataError(Exception):
    pass


if __name__ == '__main__':
    sys.exit(main())
